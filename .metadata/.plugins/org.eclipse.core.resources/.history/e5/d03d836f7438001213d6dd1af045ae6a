package pigGene;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Properties;

import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.GnuParser;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.InputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.pig.PigException;
import org.apache.pig.ResourceSchema;
import org.apache.pig.ResourceSchema.ResourceFieldSchema;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.builtin.PigStorage;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.apache.pig.impl.util.CastUtils;
import org.apache.pig.impl.util.StorageUtil;
import org.apache.pig.impl.util.UDFContext;
import org.apache.pig.impl.util.Utils;
import org.apache.pig.parser.ParserException;

public class PigGeneStorageUnmerged extends PigStorage {
	private final static CommandLineParser parser = new GnuParser();
	private final TupleFactory mTupleFactory = TupleFactory.getInstance();
	private final Options validOptions = new Options();
	boolean dontLoadSchema = false;
	private byte fieldDel = '\t';
	private ArrayList<Object> mProtoTuple = null;
	private final CommandLine configuredOptions;
	static ResourceSchema schema;

	// Indicates whether the input file path should be read.
	private boolean tagSource = false;
	private static final String TAG_SOURCE_PATH = "tagsource";
	private final Path sourcePath = null;

	static {
		final ArrayList<FieldSchema> fieldSchemaList = new ArrayList<FieldSchema>();
		fieldSchemaList.add(new FieldSchema("chrom", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("pos", org.apache.pig.data.DataType.LONG));
		fieldSchemaList.add(new FieldSchema("id", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("ref", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("alt", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("qual", org.apache.pig.data.DataType.DOUBLE));
		fieldSchemaList.add(new FieldSchema("filt", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("info", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("format", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("genotype", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("file", org.apache.pig.data.DataType.CHARARRAY));
		schema = new ResourceSchema(new Schema(fieldSchemaList));
	}

	public PigGeneStorageUnmerged() {
		this("\t", "");
	}

	public PigGeneStorageUnmerged(String delimiter) {
		this(delimiter, "");
	}

	public PigGeneStorageUnmerged(String delimiter, String options) {
		validOptions.addOption(TAG_SOURCE_PATH, false, "Appends input source file path to end of each tuple. Make sure to set pig.splitCombination to false");
		fieldDel = StorageUtil.parseFieldDel(delimiter);
		String[] optsArr = options.split(" ");
		try {
			configuredOptions = parser.parse(validOptions, optsArr);
			tagSource = configuredOptions.hasOption(TAG_SOURCE_PATH);
		} catch (ParseException e) {
			HelpFormatter formatter = new HelpFormatter();
			formatter.printHelp("PigStorage(',', '[options]')", validOptions);
			throw new RuntimeException(e);
		}
	}

	@SuppressWarnings("rawtypes")
	@Override
	public InputFormat getInputFormat() {
		return new PigGeneInputFormatUnmerged();
	}

	@Override
	public Tuple getNext() throws IOException {
		mProtoTuple = new ArrayList<Object>();
		// if (!mRequiredColumnsInitialized) {
		// if (signature != null) {
		// Properties p =
		// UDFContext.getUDFContext().getUDFProperties(this.getClass());
		// mRequiredColumns = (boolean[])
		// ObjectSerializer.deserialize(p.getProperty(signature));
		// }
		// mRequiredColumnsInitialized = true;
		// }

		// Prepend input source path if source tagging is enabled
		if (tagSource) {
			mProtoTuple.add(new DataByteArray(sourcePath.getName()));
		}

		try {
			boolean notDone = in.nextKeyValue();
			if (!notDone) {
				return null;
			}
			Text value = (Text) in.getCurrentValue();
			byte[] buf = value.getBytes();
			int len = value.getLength();
			int start = 0;
			int fieldID = 0;
			for (int i = 0; i < len; i++) {
				if (buf[i] == fieldDel) {
					if (mRequiredColumns == null || (mRequiredColumns.length > fieldID && mRequiredColumns[fieldID]))
						readField(buf, start, i);
					start = i + 1;
					fieldID++;
				}
			}
			// pick up the last field
			if (start <= len && (mRequiredColumns == null || (mRequiredColumns.length > fieldID && mRequiredColumns[fieldID]))) {
				readField(buf, start, len);
			}
			Tuple t = mTupleFactory.newTupleNoCopy(mProtoTuple);

			return dontLoadSchema ? t : applySchema(t);
		} catch (InterruptedException e) {
			int errCode = 6018;
			String errMsg = "Error while reading input";
			throw new ExecException(errMsg, errCode, PigException.REMOTE_ENVIRONMENT, e);
		}
	}

	private void readField(byte[] buf, int start, int end) {
		if (start == end) {
			// NULL value
			mProtoTuple.add(null);
		} else {
			mProtoTuple.add(new DataByteArray(buf, start, end));
		}
	}

	private Tuple applySchema(Tuple tup) throws IOException {
		if (caster == null) {
			caster = getLoadCaster();
		}
		if (signature != null && schema == null) {
			Properties p = UDFContext.getUDFContext().getUDFProperties(this.getClass(), new String[] { signature });
			String serializedSchema = p.getProperty(signature + ".schema");
			if (serializedSchema == null)
				return tup;
			try {
				schema = new ResourceSchema(Utils.getSchemaFromString(serializedSchema));
			} catch (ParserException e) {
				mLog.error("Unable to parse serialized schema " + serializedSchema, e);
			}
		}

		if (schema != null) {

			ResourceFieldSchema[] fieldSchemas = schema.getFields();
			int tupleIdx = 0;
			// If some fields have been projected out, the tuple
			// only contains required fields.
			// We walk the requiredColumns array to find required fields,
			// and cast those.
			for (int i = 0; i < fieldSchemas.length; i++) {
				if (mRequiredColumns == null || (mRequiredColumns.length > i && mRequiredColumns[i])) {
					Object val = null;
					if (tup.get(tupleIdx) != null) {
						byte[] bytes = ((DataByteArray) tup.get(tupleIdx)).get();
						val = CastUtils.convertToType(caster, bytes, fieldSchemas[i], fieldSchemas[i].getType());
					}
					tup.set(tupleIdx, val);
					tupleIdx++;
				}
			}
		}
		return tup;
	}

	@Override
	public ResourceSchema getSchema(final String location, final Job job) throws IOException {
		final Properties p = UDFContext.getUDFContext().getUDFProperties(this.getClass(), new String[] { signature });
		p.setProperty(signature + ".schema", schema.toString());
		return schema;
	}

}