package pigGene;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Properties;

import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.GnuParser;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.InputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.pig.PigException;
import org.apache.pig.ResourceSchema;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.builtin.PigStorage;
import org.apache.pig.data.DataByteArray;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.impl.logicalLayer.schema.Schema.FieldSchema;
import org.apache.pig.impl.util.ObjectSerializer;
import org.apache.pig.impl.util.StorageUtil;
import org.apache.pig.impl.util.UDFContext;

public class PigGeneStorageUnmerged extends PigStorage {
	private final static CommandLineParser parser = new GnuParser();
	private final Options validOptions = new Options();
	private byte fieldDel = '\t';
	private ArrayList<Object> mProtoTuple = null;
	private final CommandLine configuredOptions;
	static ResourceSchema schema;

	// Indicates whether the input file path should be read.
	private boolean tagSource = false;
	private static final String TAG_SOURCE_PATH = "tagsource";
	private final Path sourcePath = null;

	static {
		final ArrayList<FieldSchema> fieldSchemaList = new ArrayList<FieldSchema>();
		fieldSchemaList.add(new FieldSchema("chrom", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("pos", org.apache.pig.data.DataType.LONG));
		fieldSchemaList.add(new FieldSchema("id", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("ref", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("alt", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("qual", org.apache.pig.data.DataType.DOUBLE));
		fieldSchemaList.add(new FieldSchema("filt", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("info", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("format", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("genotype", org.apache.pig.data.DataType.CHARARRAY));
		fieldSchemaList.add(new FieldSchema("file", org.apache.pig.data.DataType.CHARARRAY));
		schema = new ResourceSchema(new Schema(fieldSchemaList));
	}

	public PigGeneStorageUnmerged() {
		this("\t", "");
	}

	public PigGeneStorageUnmerged(String delimiter) {
		this(delimiter, "");
	}

	public PigGeneStorageUnmerged(String delimiter, String options) {
		validOptions.addOption(TAG_SOURCE_PATH, false, "Appends input source file path to end of each tuple. Make sure to set pig.splitCombination to false");
		fieldDel = StorageUtil.parseFieldDel(delimiter);
		String[] optsArr = options.split(" ");
		try {
			configuredOptions = parser.parse(validOptions, optsArr);
			tagSource = configuredOptions.hasOption(TAG_SOURCE_PATH);
		} catch (ParseException e) {
			HelpFormatter formatter = new HelpFormatter();
			formatter.printHelp("PigStorage(',', '[options]')", validOptions);
			throw new RuntimeException(e);
		}
	}

	@SuppressWarnings("rawtypes")
	@Override
	public InputFormat getInputFormat() {
		return new PigGeneInputFormatUnmerged();
	}

	@Override
	public Tuple getNext() throws IOException {
		mProtoTuple = new ArrayList<Object>();
//		if (!mRequiredColumnsInitialized) {
//			if (signature != null) {
//				Properties p = UDFContext.getUDFContext().getUDFProperties(this.getClass());
//				mRequiredColumns = (boolean[]) ObjectSerializer.deserialize(p.getProperty(signature));
//			}
//			mRequiredColumnsInitialized = true;
//		}
		
		// Prepend input source path if source tagging is enabled
		if (tagSource) {
			mProtoTuple.add(new DataByteArray(sourcePath.getName()));
		}

		try {
			boolean notDone = in.nextKeyValue();
			if (!notDone) {
				return null;
			}
			Text value = (Text) in.getCurrentValue();
			byte[] buf = value.getBytes();
			int len = value.getLength();
			int start = 0;
			int fieldID = 0;
			for (int i = 0; i < len; i++) {
				if (buf[i] == fieldDel) {
					if (mRequiredColumns == null || (mRequiredColumns.length > fieldID && mRequiredColumns[fieldID]))
						readField(buf, start, i);
					start = i + 1;
					fieldID++;
				}
			}
			// pick up the last field
			if (start <= len && (mRequiredColumns == null || (mRequiredColumns.length > fieldID && mRequiredColumns[fieldID]))) {
				readField(buf, start, len);
			}
			Tuple t = mTupleFactory.newTupleNoCopy(mProtoTuple);

			return dontLoadSchema ? t : applySchema(t);
		} catch (InterruptedException e) {
			int errCode = 6018;
			String errMsg = "Error while reading input";
			throw new ExecException(errMsg, errCode, PigException.REMOTE_ENVIRONMENT, e);
		}
	}
	
	private void readField(byte[] buf, int start, int end) {
        if (start == end) {
            // NULL value
            mProtoTuple.add(null);
        } else {
            mProtoTuple.add(new DataByteArray(buf, start, end));
        }
    }

	@Override
	public ResourceSchema getSchema(final String location, final Job job) throws IOException {
		final Properties p = UDFContext.getUDFContext().getUDFProperties(this.getClass(), new String[] { signature });
		p.setProperty(signature + ".schema", schema.toString());
		return schema;
	}

}